\documentclass[openany]{book}
%% PAQUETES %%
\usepackage[utf8]{inputenc}
\usepackage{titling}
\usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{float}
\usepackage{amssymb}
%\usepackage{ulem}
\usepackage{ragged2e}
\usepackage{fancyhdr}
\usepackage{hyphenat}
\usepackage{titlesec}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{cancel}


\setlength{\textwidth}{130mm}
\setlength{\textheight}{190mm}
\graphicspath{img/}


%% RE ESTILIZACION DE COMNADOS
\titleformat
{\chapter} % command
[display] % shape
{\sc\Large} % format
{Unidad \thechapter} % label
{0.1pt} % sep
{
    \rule{\textwidth}{0.5pt}
    \vspace{0.2ex}
    \centering
} % before-code
\renewcommand{\labelitemi}{$\succ$}
\renewcommand{\chaptermark}[1]{\markboth{\sc #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\sc #1}}

\fancyhf{}
\lhead[\thepage]{\rightmark}
\rhead[\leftmark]{\thepage}
\renewcommand{\headrulewidth}{0.1mm}
\cfoot{{\it Estadística Aplicada}}
\renewcommand{\footrulewidth}{0.1mm}

\allowhyphens

\begin{document}


\begin{titlepage}
	\centering
	{\bfseries\LARGE Universidad Nacional de La Plata \par}
	\vspace{1cm}
	{\scshape\Large Facultad de Ciencias Astronómicas y Geofíscias \par}
	\vspace{3cm}
	{\scshape\Huge Estadística Aplicada \par}
	\vspace{3cm}
	{\itshape\Large Resumen teórico \par}
	\vfill
	{\Large Autor: \par}
	{\Large Lorenzo Girotti \par}
	\vfill
	{\Large 2024 \par}
\end{titlepage}
\sffamily
\chapter{Probabilidad}
\section{Definiciones}
Antes que nada, vamos a definir conceptos que utilizaremos a lo largo del resumen.
\begin{itemize}
  \item \textbf{Variable aleatoria:} Es el resultado de un experimento aleatorio, que puede ser por ejemplo la acción de lanzar un dado y ver qué número queda en la cara superior. En notación, diremos que X es una variable aleatoria y la denotaremos por $\mathrm{X}$.
  \item \textbf{Variable determinista:} Es un valor que podemos \emph{determinar} (valga la redundancia) y usualmente asumiremos que se comportan como números reales o subconjuntos de los mismos. Las notamos como $x$.
  \item \textbf{Espacio muestral o universo:} Representa la totalidad de los posibles resultados de un experimento aleatorio, normalmente lo notaremos como $E$ o $U$.
  \item \textbf{Eventos mutuamente excluyentes:} Un \emph{evento} lo podemos pensar como un suceso o algo que puede ocurrir. Cuando tenemos dos o más eventos que tienen la característica de que no pueden coexistir o suceder al mismo tiempo decimos que son \emph{mutuamente excluyentes}. Por ejemplo: \sf{sea el evento A que salga cruz; y el evento B que salga cara. A y B son eventos mutuamente excluyentes en el experimento de lanzar una moneda}.
  \item \rm{\textbf{Probabilidad empírica:} Dado un evento que sea resultado de un experimento aleatorio que consiste en $n$ pruebas, definimos la probabilidad de que ocurra ese evento como:
  \begin{equation}
    \label{eq:prob-emp}
    P(\mathrm{X})=\frac{\#X}{n}
  \end{equation}
		siendo $\#X$ la cantidad de veces que ocurre el evento aleatorio $\mathrm{X}$.} 
\end{itemize}
\section{Reglas de probabilidad y resultados importantes}
  \begin{itemize}
  \item \textbf{Probabilidad aditiva:} \begin{equation}
	P(A)+P(B)-P(A\cap B)
	\label{eq:prob-aditiva}
  .\end{equation}
  en caso de que sean \emph{mutuamente excluyentes}, $P(A\cap B)=0$
  \item \textbf{Probabilidad conjunta:} \begin{equation}
	P(A\cap B)=P(A|B)P(B)
	\label{eq:prob-conj}
  \end{equation}
  si $A$ y $B$ son \emph{estadísticamente independientes}, entonces $P(A|B)=P(A)$ y $P(A\cap B)=P(A)P(B)$.
  \item \textbf{Probabilidad total:} Dado un espacio muestral compuesto de $n$ subconjuntos: $E=A_1\cup\cdots\cup A_{n}$, la probabilidad de que ocurra un evento $B$ dentro del espacio, es \begin{equation}
	P(B)=\sum_{i=1}^{n}P(B\cap A_{i})
  ,\end{equation}
  considerando que es la unión de todas las intersecciones entre $B$ y los subconjuntos $A_{i}$ y asumiendo que los mismos son \emph{mutuamente excluyentes}. Si tenemos en cuenta la \refeq{eq:prob-conj}, la probabilidad total queda \begin{equation}
	P(B)=\sum_{i=1}^{n}P(B|A_{i})P(A_{i})
	\label{eq:prob-total}
  .\end{equation}
  \item \textbf{Teorema de Bayes:} Sean $A_{i}$ como en el ítem anterior, \begin{equation}
	P(A_{i}|B)=\frac{P(B|A_{i})P(A_{i})}{P(B)}
	\label{eq:bayes}
  \end{equation}
\end{itemize}

\chapter{Variable aleatoria}

\section{Función de distribución de una variable aleatoria}
Si consideramos una variable aleatoria discreta, definimos a su \emph{función de distribución} como 
\begin{equation}
	\label{eq:func-distr-discreta}
	P(\mathrm{X}<x)=\sum_{i=1}^{k(x_k<x)}P(\mathrm{X}=x_i)
.\end{equation}
\par Si la variable aleatoria es continua, se define la función de densidad lineal de probabilidad:
\begin{equation}
  f(x)dx=P(x\leq\mathrm{X}<x+dx)
  \label{eq:densidad-de-prob}
\end{equation}
Notar que para una función continua no tiene sentido hablar de la probabilidad de que $\rm{X}$ sea igual a un valor determinado. Sí diremos que la variable aleatoria se encuentra en un intervalo infinitesimal de posibilidades.
\par Podemos relacionar la \emph{densidad de probabilidad} con la \emph{distribución} de la variable a través de:
\begin{equation}
  \label{eq:fyF}
  f(x)=\frac{dF(x)}{dx}
\end{equation}.
Tener en cuenta que 
\begin{equation*}
  f(x)\geq0;\qquad \int_{-\infty}^{\infty}f(x)dx=1
\end{equation*}.

\par De esta manera, podemos calcular la probabilidad acumulada de una variable aleatoria continua a través de su función de densidad de probabilidad.
\begin{gather}
  \label{eq:Fx-ft}
  F(x)=\int_{-\infty}^{x}f(t)dt.\\
  P(x_1<\mathrm{X}<x_2)=F(x_2)-F(x_1)=\int_{x_1}^{x_2}f(x)dx.
\end{gather}
Notar que
\begin{equation*}
  F(-\infty)=\int_{-\infty}^{-\infty}f(x)dx=0;\qquad F(\infty)=\int_{-\infty}^{\infty}f(x)dx=1.
\end{equation*}

\section{Operador Esperanza y parámetros de variables aleatorias}
Definimos al \emph{operador esperanza} como:
\begin{equation*}
  E[H(\mathrm{X})]=\sum_{i=1}^{n}H(x_i)P(\mathrm{X}=x_i)
\end{equation*}
si X es discreta.
\begin{equation*}
  E\left[H(\mathrm{X})\right]=\int_{-\infty}^{\infty}H(x)f(x)dx
\end{equation*}
si X es continua.
\par El operador es \emph{lineal}, es decir:
\begin{gather*}
  E\left[\mathrm{X}+\mathrm{Y}\right]=E\left[\mathrm{X}\right]+E\left[\mathrm{Y}\right]\\
  E\left[c\mathrm{X}\right]=cE\left[\mathrm{X}\right],
\end{gather*}
donde $c$ es una constante.

\subsection{Momentos de orden}
Dado un $H(\mathrm{X})=(\mathrm{X}-c)^{l}$, el valor esperado $E\left[H(\mathrm{X})\right]$ se llama \emph{momento de orden $l$} de una variable con respecto a $c$.
\par Si tomamos a $c=\mu$ siendo $\mu$ la media de una variable, definimos $\mu_{l}=E\left[(\mathrm{X}-\mu)^{l}\right]$ y tenemos:

\begin{align*}
  \mu_0=1\quad &\\
  \mu_1=0\quad &\\
  \mu_2=E\left[(\mathrm{X}-\mu)^2\right]=\sigma^2\quad &\mathrm{Varianza}\\
  \mu_3=E\left[(\mathrm{X}-\mu)^3\right]\quad &\textrm{Sesgo de X}\\
  \mu_4=E\left[(\mathrm{X}-\mu)^4\right]\quad &\textrm{Curtosis de X}\\
  \vdots\quad &
\end{align*}

Nota: $E\left[\mathrm{X}\right]=\mu$

\par Cada \emph{momento} define mejor el comportamiento de la variable.

\subsection{Variable normalizada}
Una variable normalizada es aquella que tiene media cero y varianza uno.
\begin{equation}
  \label{eq:variable-normalizada}
  \mathrm{u}=\frac{\mathrm{X}-\mu}{\sigma_{\mathrm{X}}}
\end{equation}

\subsection{Desigualdad de Chebychev}
\begin{equation}
  \label{eq:Chebychev}
  P(\mathrm{|\mathrm{x}-\mu|>k\sigma})\leq\frac{1}{k^2}
\end{equation}
siendo $k\in\mathbb{R}$.
\par La desigualdad nos da una idea probabilística de nuestra variable $\mathrm{X}$, conociendo solo su media y varianza (sin importar cual es su función de probabilidad o probabilidad asociada). Por ejemplo: si elegimos $k=3$, tenemos el \emph{criterio de los tres sigmas} para descartar \emph{outlayers}.

\section{Transformación de variables}
Si tenemos una transformación de variable $\mathrm{Y(X)}$ biyectiva, \emph{la probabilidad en intervalos equivalentes se conserva}, esto implica
\begin{gather*}
  P(x\leq\mathrm{X}<x+dx)=P(y\leq\mathrm{Y}<y+dy)\\
  f(x)dx=g(y)dy
\end{gather*}.
\par Entonces podemos definir la densidad de probabilidad de $g$ como
\begin{equation}
  \label{eq:gdy}
  g(y)=\left|\frac{dx}{dy}\right|f(x)
\end{equation}.
Gráficamente podemos decir que hay una \emph{igualdad de áreas}.

\section{Multivariables}
Todo el análisis que se realiza para una variable aleatoria, puede extenderse a una idea multivariable.

\subsection{Dos variables}
Siendo dos variables continuas, tenemos:
\subsubsection*{Función de densidad de probabilidad conjunta}
\begin{equation}
  f(x,y)=\frac{\partial }{\partial x}\frac{\partial }{\partial y}F(x,y),
  \label{eq:f_xy}
\end{equation}
y,
\begin{equation}
  P(a\leq\mathrm{X}<b,c\leq\mathrm{y}<d)=\int_{a}^{b}\left[\int_{c}^{d}f(x,y)dy\right]dx.
\end{equation}

\subsubsection*{Función de densidad de probabilidad marginal}
Se consideran todos los valores posibles para una variable, dejando una densidad de probabilidad con única dependencia de la otra.
\begin{equation}
  \label{eq:marginal}
  g(x)=\int_{-\infty}^{\infty}f(x,y)dy
\end{equation}
tenemos entonces,
\begin{equation}
  P(a\leq\mathrm{X}<b,-\infty<y<\infty)=\int_{a}^{b}\left[\int_{-\infty}^{\infty}f(x,y)dy\right]dx=\int_{a}^{b}g(x)dx
\end{equation}

\subsubsection*{Independencia de variables}
Dos varaibles se dicen independientes \emph{estadísticamente} si
\begin{equation}
  \label{eq:independent}
  f(x,y)=g(x)h(y)
\end{equation}
donde $g(x)$ y $h(y)$ son las funciones de densidad marginales.

\subsubsection*{Probabilidad condicional}
Partimos de la base de encontrar:
\begin{equation*}
  P(y\leq\mathrm{Y}<y+dy\,|\,x\leq\mathrm{X}<x+dx)
\end{equation*}

La función de densidad de probabilidad de Y si X:
\begin{equation}
  \label{eq:densidad-condicional}
  f(y|x)=\frac{f(x,y)}{g(x)}
\end{equation}

\subsubsection*{Parámetros}
\begin{itemize}
  \item \textbf{Valor esperado:} $$E\left[H(\mathrm{x,y})\right]=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}H(x,y)f(x,y)dxdy$$
  \item \textbf{Varianza conjunta:} $$\sigma^2\left[H(\mathrm{x,y})\right]=E\left[\left(H(\mathrm{x,y})-E\left[H(\mathrm{x,y})\right]\right)^2\right]$$
  \item \textbf{Medias y varianzas para $\mathrm{x}$ (ídem para $\mathrm{y}$):} \begin{gather*}
      E\left[\mathrm{x}\right]=\int_{-\infty}^{\infty}xg(x)dx=\mu_{\mathrm{x}}\\ 
      \sigma^2_{\mathrm{x}}=E\left[(\mathrm{x}-\mu_{\mathrm{x}})^2\right]=\int_{-\infty}^{\infty}(x-\mu_\mathrm{x})^2g(x)dx
    \end{gather*}
  \item \textbf{Covarianza:} \begin{align}
      \label{eq:covarianza}
      \mathrm{cov(x,y)}&=E\left[(\mathrm{x}-\mu_\mathrm{x})(\mathrm{y}-\mu_\mathrm{y})\right]=\notag \\
      &=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}(x-\mu_\mathrm{x})(y-\mu_\mathrm{y})f(x,y)dxdy
    \end{align}
  \item \textbf{Coeficiente de correlación:} \begin{equation}
    \label{eq:correlacion}
    \rho(\mathrm{x,y})=\frac{\mathrm{cov(x,y)}}{\sigma(\mathrm{x})\sigma(\mathrm{y})}
  \end{equation}
\end{itemize}

\par Como vemos, se hace una extensión de los parámetros que ya conocíamos; con la diferencia de que ahora la dependencia estadística entre las variables se vuelve de gran interés. La \emph{covarianza} \refeq{eq:covarianza} es un parámetro ideal que permite analizar la dependencia estadística en conjunto con la \emph{correlación} \refeq{eq:correlacion}. Algunas de las características más importantes de la correlación son las siguientes:

\begin{enumerate}
  \item $-1\leq\rho(\mathrm{x,y})\leq1$
  \item Si $\mathrm{x}$ e $\mathrm{y}$ son variables estadísticamente independientes, entonces $\rho(\mathrm{x,y})=0$
  \item Si $\mathrm{y}$ es una función determinsta de $\mathrm{x}$, $\mathrm{y}=H(\mathrm{x})$, entonces $\rho(\mathrm{x,y})=\pm1$
\end{enumerate}

\subsection{Transformación de variables con dos variables}
Sean las variables $u$ y $v$ funciones deterministas de las variables aleatorias $\mathrm{x}$ e $\mathrm{y}$, de forma que $u=u(\mathrm{x,y})$ y $v=(\mathrm{x,y})$. Para realizar la transformación, seguimos el mismo criterio de \emph{igualdad de áreas} solo que ahora tenemos una dependencia de dos variables. Por lo tanto, de ahora en adelante tendremos que utilizar el \emph{Jacobiano}.
\par Conociendo $f(x,y)$ determinamos $g(u,v)$ a través de: \begin{equation}
  \label{eq:trans-dos-variables}
  g(u,v)=f(x(u,v),y(u,v))\left|J\left(\frac{x,y}{u,v}\right)\right|
\end{equation}

\subsection{Caso de n variables}
Definimos un vector de varaibles aleatorias $\vec{\mathrm{x}}=(\mathrm{x}_1,\dots,\mathrm{x}_{n})=(\mathrm{x}_1\,\cdots\,\mathrm{x}_{n})^{T}$ y entonces quedan:

\subsubsection*{Función de distribución conjunta:}
\begin{equation*}
  F(x_1,\dots,x_n)=P(\mathrm{x}_1<x_1,\dots,\mathrm{x}_{n}<x_n)
\end{equation*}
\subsubsection*{Función de densidad de probabilidad:}
\begin{equation*}
  f(x_1,\dots,x_n)=\frac{\partial^n}{\partial x_1\cdots\partial x_{n}}F(x_1,\dots,x_{n})
\end{equation*}
\subsubsection*{Función de densidad de probabilidad marginal:}
\begin{equation*}
  g_{r}(x_{r})=\int_{-\infty}^{\infty}\cdots \int_{-\infty}^{\infty}f(x_1,\dots,x_n)dx_{1}\cdots dx_{r-1}dx_{r+1}\cdots dx_{n}
\end{equation*}
\subsubsection*{Operador Esperanza:}
\begin{equation*}
  E\left[H(\vec{\mathrm{x}})\right]=\int_{-\infty}^{\infty}\cdots \int_{-\infty}^{\infty}f(x_1,\dots,x_{n})dx_{1}\cdots dx_{n}
\end{equation*}
\subsubsection*{Media de una variable en particular:}
\begin{gather*}
  E\left[\mathrm{x}_r\right]=\int_{-\infty}^{\infty}\cdots \int_{-\infty}^{\infty}x_{r}f(x_1,\dots,x_{n})dx_{1}\cdots dx_{n}\\
  E\left[\mathrm{x}_r\right]=\int_{-\infty}^{\infty}x_{r}g_{r}(x_{r})dx_{r}
\end{gather*}

\subsubsection*{Independencia de variables:}
Si las variables son independientes, tenemos
\begin{equation*}
  f(x_{1},\dots,x_{n})=g_{1}(x_{1})\cdots g_{n}(x_{n})
\end{equation*}

\subsection{Vector de media, varianza y covarianza}
Si coleccionamos todas las medias de las variables $(\mu_{\mathrm{x}_r})$ obtenemos un vector de medias dado por:
\begin{equation}
  \label{media-vec}
  \vec{\mu}_{\vec{\mathrm{x}}}=(\mu_{\mathrm{x}_1},\dots,\mu_{\mathrm{x}_n})=(E\left[\mathrm{x}_1\right]\,\cdots\,E\left[\mathrm{x}_n\right])^T
\end{equation}

\par Si aplicamos lo mismo para intentar calcular la varianza; obtenemos la \emph{matriz de varianza-covarianza}

\begin{equation*}
  E\left[(\vec{\mathrm{x}}-\vec{\mu}_{\vec{\mathrm{x}}})(\vec{\mathrm{x}}-\vec{\mu}_{\vec{\mathrm{x}}})^T\right]=\begin{bmatrix} E\left[(\mathrm{x_1}-\mu_1)^2\right] & \cdots & E\left[(\mathrm{x_1}-\mu_1)(\mathrm{x}_{n}-\mu_{n})\right]\\
    \vdots & \ddots & \vdots \\
  E\left[(\mathrm{x}_{n}-\mu_{n})(\mathrm{x_{1}}-\mu_{1})\right] & \cdots & E\left[(\mathrm{x}_{n}-\mu_{n})^2\right] \end{bmatrix}
\end{equation*}

que utilizando las definiciones de varianza y covariazna queda

\begin{equation}
  \label{eq:matriz-var-cov}
  E\left[(\vec{\mathrm{x}}-\vec{\mu}_{\vec{\mathrm{x}}})(\vec{\mathrm{x}}-\vec{\mu}_{\vec{\mathrm{x}}})^T\right]=\begin{bmatrix} \sigma^{2}_{\mathrm{x}_1} & \cdots & \mathrm{cov(x_{1},x_{n})} \\
    \vdots & \ddots & \vdots \\
    \mathrm{cov(x_{n},x_{1})} & \cdots & \sigma^{2}_{\mathrm{x}_n} \end{bmatrix}
\end{equation}

\end{document}


