\documentclass[openany]{book}
%% PAQUETES %%
\usepackage[utf8]{inputenc}
\usepackage{titling}
\usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{float}
\usepackage{amssymb}
%\usepackage{ulem}
\usepackage{ragged2e}
\usepackage{fancyhdr}
\usepackage{hyphenat}
\usepackage{titlesec}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{cancel}


\setlength{\textwidth}{130mm}
\setlength{\textheight}{190mm}
\graphicspath{img/}


%% RE ESTILIZACION DE COMNADOS
\titleformat
{\chapter} % command
[display] % shape
{\sc\Large} % format
{Unidad \thechapter} % label
{0.1pt} % sep
{
    \rule{\textwidth}{0.5pt}
    \vspace{0.2ex}
    \centering
} % before-code
\renewcommand{\labelitemi}{$\succ$}
\renewcommand{\chaptermark}[1]{\markboth{\sc #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\sc #1}}

\fancyhf{}
\lhead[\thepage]{\rightmark}
\rhead[\leftmark]{\thepage}
\renewcommand{\headrulewidth}{0.1mm}
\cfoot{{\it Estadística Aplicada}}
\renewcommand{\footrulewidth}{0.1mm}

\allowhyphens

\begin{document}


\begin{titlepage}
	\centering
	{\bfseries\LARGE Universidad Nacional de La Plata \par}
	\vspace{1cm}
	{\scshape\Large Facultad de Ciencias Astronómicas y Geofíscias \par}
	\vspace{3cm}
	{\scshape\Huge Estadística Aplicada \par}
	\vspace{3cm}
	{\itshape\Large Resumen teórico \par}
	\vfill
	{\Large Autor: \par}
	{\Large Lorenzo Girotti \par}
	\vfill
	{\Large 2024 \par}
\end{titlepage}
\sffamily
\chapter{Probabilidad}
\section{Definiciones}
Antes que nada, vamos a definir conceptos que utilizaremos a lo largo del resumen.
\begin{itemize}
  \item \textbf{Variable aleatoria:} Es el resultado de un experimento aleatorio, que puede ser por ejemplo la acción de lanzar un dado y ver qué número queda en la cara superior. En notación, diremos que X es una variable aleatoria y la denotaremos por $\mathrm{X}$.
  \item \textbf{Variable determinista:} Es un valor que podemos \emph{determinar} (valga la redundancia) y usualmente asumiremos que se comportan como números reales o subconjuntos de los mismos. Las notamos como $x$.
  \item \textbf{Espacio muestral o universo:} Representa la totalidad de los posibles resultados de un experimento aleatorio, normalmente lo notaremos como $E$ o $U$.
  \item \textbf{Eventos mutuamente excluyentes:} Un \emph{evento} lo podemos pensar como un suceso o algo que puede ocurrir. Cuando tenemos dos o más eventos que tienen la característica de que no pueden coexistir o suceder al mismo tiempo decimos que son \emph{mutuamente excluyentes}. Por ejemplo: \sf{sea el evento A que salga cruz; y el evento B que salga cara. A y B son eventos mutuamente excluyentes en el experimento de lanzar una moneda}.
  \item \rm{\textbf{Probabilidad empírica:} Dado un evento que sea resultado de un experimento aleatorio que consiste en $n$ pruebas, definimos la probabilidad de que ocurra ese evento como:
  \begin{equation}
    \label{eq:prob-emp}
    P(\mathrm{X})=\frac{\#X}{n}
  \end{equation}
		siendo $\#X$ la cantidad de veces que ocurre el evento aleatorio $\mathrm{X}$.} 
\end{itemize}
\section{Reglas de probabilidad y resultados importantes}
  \begin{itemize}
  \item \textbf{Probabilidad aditiva:} \begin{equation}
	P(A)+P(B)-P(A\cap B)
	\label{eq:prob-aditiva}
  .\end{equation}
  en caso de que sean \emph{mutuamente excluyentes}, $P(A\cap B)=0$
  \item \textbf{Probabilidad conjunta:} \begin{equation}
	P(A\cap B)=P(A|B)P(B)
	\label{eq:prob-conj}
  \end{equation}
  si $A$ y $B$ son \emph{estadísticamente independientes}, entonces $P(A|B)=P(A)$ y $P(A\cap B)=P(A)P(B)$.
  \item \textbf{Probabilidad total:} Dado un espacio muestral compuesto de $n$ subconjuntos: $E=A_1\cup\cdots\cup A_{n}$, la probabilidad de que ocurra un evento $B$ dentro del espacio, es \begin{equation}
	P(B)=\sum_{i=1}^{n}P(B\cap A_{i})
  ,\end{equation}
  considerando que es la unión de todas las intersecciones entre $B$ y los subconjuntos $A_{i}$ y asumiendo que los mismos son \emph{mutuamente excluyentes}. Si tenemos en cuenta la \refeq{eq:prob-conj}, la probabilidad total queda \begin{equation}
	P(B)=\sum_{i=1}^{n}P(B|A_{i})P(A_{i})
	\label{eq:prob-total}
  .\end{equation}
  \item \textbf{Teorema de Bayes:} Sean $A_{i}$ como en el ítem anterior, \begin{equation}
	P(A_{i}|B)=\frac{P(B|A_{i})P(A_{i})}{P(B)}
	\label{eq:bayes}
  \end{equation}
\end{itemize}

\chapter{Variable aleatoria}

\section{Función de distribución de una variable aleatoria}
Si consideramos una variable aleatoria discreta, definimos a su \emph{función de distribución} como 
\begin{equation}
	\label{eq:func-distr-discreta}
	P(\mathrm{X}<x)=\sum_{i=1}^{k(x_k<x)}P(\mathrm{X}=x_i)
.\end{equation}
\par Si la variable aleatoria es continua, se define la función de densidad lineal de probabilidad:
\begin{equation}
  f(x)dx=P(x\leq\mathrm{X}<x+dx)
  \label{eq:densidad-de-prob}
\end{equation}
Notar que para una función continua no tiene sentido hablar de la probabilidad de que $\rm{X}$ sea igual a un valor determinado. Sí diremos que la variable aleatoria se encuentra en un intervalo infinitesimal de posibilidades.
\par Podemos relacionar la \emph{densidad de probabilidad} con la \emph{distribución} de la variable a través de:
\begin{equation}
  \label{eq:fyF}
  f(x)=\frac{dF(x)}{dx}
\end{equation}.
Tener en cuenta que 
\begin{equation*}
  f(x)\geq0;\qquad \int_{-\infty}^{\infty}f(x)dx=1
\end{equation*}.

\par De esta manera, podemos calcular la probabilidad acumulada de una variable aleatoria continua a través de su función de densidad de probabilidad.
\begin{gather}
  \label{eq:Fx-ft}
  F(x)=\int_{-\infty}^{x}f(t)dt.\\
  P(x_1<\mathrm{X}<x_2)=F(x_2)-F(x_1)=\int_{x_1}^{x_2}f(x)dx.
\end{gather}
Notar que
\begin{equation*}
  F(-\infty)=\int_{-\infty}^{-\infty}f(x)dx=0;\qquad F(\infty)=\int_{-\infty}^{\infty}f(x)dx=1.
\end{equation*}

\section{Operador Esperanza y parámetros de variables aleatorias}
Definimos al \emph{operador esperanza} como:
\begin{equation*}
  E[H(\mathrm{X})]=\sum_{i=1}^{n}H(x_i)P(\mathrm{X}=x_i)
\end{equation*}
si X es discreta.
\begin{equation*}
  E\left[H(\mathrm{X})\right]=\int_{-\infty}^{\infty}H(x)f(x)dx
\end{equation*}
si X es continua.
\par El operador es \emph{lineal}, es decir:
\begin{gather*}
  E\left[\mathrm{X}+\mathrm{Y}\right]=E\left[\mathrm{X}\right]+E\left[\mathrm{Y}\right]\\
  E\left[c\mathrm{X}\right]=cE\left[\mathrm{X}\right],
\end{gather*}
donde $c$ es una constante.

\subsection{Momentos de orden}
Dado un $H(\mathrm{X})=(\mathrm{X}-c)^{l}$, el valor esperado $E\left[H(\mathrm{X})\right]$ se llama \emph{momento de orden $l$} de una variable con respecto a $c$.
\par Si tomamos a $c=\mu$ siendo $\mu$ la media de una variable, definimos $\mu_{l}=E\left[(\mathrm{X}-\mu)^{l}\right]$ y tenemos:

\begin{align*}
  \mu_0=1\quad &\\
  \mu_1=0\quad &\\
  \mu_2=E\left[(\mathrm{X}-\mu)^2\right]=\sigma^2\quad &\mathrm{Varianza}\\
  \mu_3=E\left[(\mathrm{X}-\mu)^3\right]\quad &\textrm{Sesgo de X}\\
  \mu_4=E\left[(\mathrm{X}-\mu)^4\right]\quad &\textrm{Curtosis de X}\\
  \vdots\quad &
\end{align*}

Nota: $E\left[\mathrm{X}\right]=\mu$

\par Cada \emph{momento} define mejor el comportamiento de la variable.

\subsection{Variable normalizada}
Una variable normalizada es aquella que tiene media cero y varianza uno.
\begin{equation}
  \label{eq:variable-normalizada}
  \mathrm{u}=\frac{\mathrm{X}-\mu}{\sigma_{\mathrm{X}}}
\end{equation}

\subsection{Desigualdad de Chebychev}
\begin{equation}
  \label{eq:Chebychev}
  P(\mathrm{|\mathrm{x}-\mu|>k\sigma})\leq\frac{1}{k^2}
\end{equation}
siendo $k\in\mathbb{R}$.
\par La desigualdad nos da una idea probabilística de nuestra variable $\mathrm{X}$, conociendo solo su media y varianza (sin importar cual es su función de probabilidad o probabilidad asociada). Por ejemplo: si elegimos $k=3$, tenemos el \emph{criterio de los tres sigmas} para descartar \emph{outlayers}.

\section{Transformación de variables}
Si tenemos una transformación de variable $\mathrm{Y(X)}$ biyectiva, \emph{la probabilidad en intervalos equivalentes se conserva}, esto implica
\begin{gather*}
  P(x\leq\mathrm{X}<x+dx)=P(y\leq\mathrm{Y}<y+dy)\\
  f(x)dx=g(y)dy
\end{gather*}.
\par Entonces podemos definir la densidad de probabilidad de $g$ como
\begin{equation}
  \label{eq:gdy}
  g(y)=\left|\frac{dx}{dy}\right|f(x)
\end{equation}.
Gráficamente podemos decir que hay una \emph{igualdad de áreas}.

\section{Multivariables}
Todo el análisis que se realiza para una variable aleatoria, puede extenderse a una idea multivariable.

\subsection{Dos variables}
Siendo dos variables continuas, tenemos:
\subsubsection*{Función de densidad de probabilidad conjunta}
\begin{equation}
  f(x,y)=\frac{\partial }{\partial x}\frac{\partial }{\partial y}F(x,y),
  \label{eq:f_xy}
\end{equation}
y,
\begin{equation}
  P(a\leq\mathrm{X}<b,c\leq\mathrm{y}<d)=\int_{a}^{b}\left[\int_{c}^{d}f(x,y)dy\right]dx.
\end{equation}

\subsubsection*{Función de densidad de probabilidad marginal}
Se consideran todos los valores posibles para una variable, dejando una densidad de probabilidad con única dependencia de la otra.
\begin{equation}
  \label{eq:marginal}
  g(x)=\int_{-\infty}^{\infty}f(x,y)dy
\end{equation}
tenemos entonces,
\begin{equation}
  P(a\leq\mathrm{X}<b,-\infty<y<\infty)=\int_{a}^{b}\left[\int_{-\infty}^{\infty}f(x,y)dy\right]dx=\int_{a}^{b}g(x)dx
\end{equation}

\subsubsection*{Independencia de variables}
Dos varaibles se dicen independientes \emph{estadísticamente} si
\begin{equation}
  \label{eq:independent}
  f(x,y)=g(x)h(y)
\end{equation}
donde $g(x)$ y $h(y)$ son las funciones de densidad marginales.

\subsubsection*{Probabilidad condicional}
Partimos de la base de encontrar:
\begin{equation*}
  P(y\leq\mathrm{Y}<y+dy\,|\,x\leq\mathrm{X}<x+dx)
\end{equation*}

La función de densidad de probabilidad de Y si X:
\begin{equation}
  \label{eq:densidad-condicional}
  f(y|x)=\frac{f(x,y)}{g(x)}
\end{equation}

\subsubsection*{Parámetros}
\begin{itemize}
  \item \textbf{Valor esperado:} $$E\left[H(\mathrm{x,y})\right]=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}H(x,y)f(x,y)dxdy$$
  \item \textbf{Varianza conjunta:} $$\sigma^2\left[H(\mathrm{x,y})\right]=E\left[\left(H(\mathrm{x,y})-E\left[H(\mathrm{x,y})\right]\right)^2\right]$$
  \item \textbf{Medias y varianzas para $\mathrm{x}$ (ídem para $\mathrm{y}$):} \begin{gather*}
      E\left[\mathrm{x}\right]=\int_{-\infty}^{\infty}xg(x)dx=\mu_{\mathrm{x}}\\ 
      \sigma^2_{\mathrm{x}}=E\left[(\mathrm{x}-\mu_{\mathrm{x}})^2\right]=\int_{-\infty}^{\infty}(x-\mu_\mathrm{x})^2g(x)dx
    \end{gather*}
  \item \textbf{Covarianza:} \begin{align}
      \label{eq:covarianza}
      \mathrm{cov(x,y)}&=E\left[(\mathrm{x}-\mu_\mathrm{x})(\mathrm{y}-\mu_\mathrm{y})\right]=\notag \\
      &=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}(x-\mu_\mathrm{x})(y-\mu_\mathrm{y})f(x,y)dxdy
    \end{align}
  \item \textbf{Coeficiente de correlación:} \begin{equation}
    \label{eq:correlacion}
    \rho(\mathrm{x,y})=\frac{\mathrm{cov(x,y)}}{\sigma(\mathrm{x})\sigma(\mathrm{y})}
  \end{equation}
\end{itemize}

\par Como vemos, se hace una extensión de los parámetros que ya conocíamos; con la diferencia de que ahora la dependencia estadística entre las variables se vuelve de gran interés. La \emph{covarianza} \refeq{eq:covarianza} es un parámetro ideal que permite analizar la dependencia estadística en conjunto con la \emph{correlación} \refeq{eq:correlacion}. Algunas de las características más importantes de la correlación son las siguientes:

\begin{enumerate}
  \item $-1\leq\rho(\mathrm{x,y})\leq1$
  \item Si $\mathrm{x}$ e $\mathrm{y}$ son variables estadísticamente independientes, entonces $\rho(\mathrm{x,y})=0$
  \item Si $\mathrm{y}$ es una función determinsta de $\mathrm{x}$, $\mathrm{y}=H(\mathrm{x})$, entonces $\rho(\mathrm{x,y})=\pm1$
\end{enumerate}

\subsection{Transformación de variables con dos variables}
Sean las variables $u$ y $v$ funciones deterministas de las variables aleatorias $\mathrm{x}$ e $\mathrm{y}$, de forma que $u=u(\mathrm{x,y})$ y $v=(\mathrm{x,y})$. Para realizar la transformación, seguimos el mismo criterio de \emph{igualdad de áreas} solo que ahora tenemos una dependencia de dos variables. Por lo tanto, de ahora en adelante tendremos que utilizar el \emph{Jacobiano}.
\par Conociendo $f(x,y)$ determinamos $g(u,v)$ a través de: \begin{equation}
  \label{eq:trans-dos-variables}
  g(u,v)=f(x(u,v),y(u,v))\left|J\left(\frac{x,y}{u,v}\right)\right|
\end{equation}

\subsection{Caso de n variables}
Definimos un vector de varaibles aleatorias $\vec{\mathrm{x}}=(\mathrm{x}_1,\dots,\mathrm{x}_{n})=(\mathrm{x}_1\,\cdots\,\mathrm{x}_{n})^{T}$ y entonces quedan:

\subsubsection*{Función de distribución conjunta:}
\begin{equation*}
  F(x_1,\dots,x_n)=P(\mathrm{x}_1<x_1,\dots,\mathrm{x}_{n}<x_n)
\end{equation*}
\subsubsection*{Función de densidad de probabilidad:}
\begin{equation*}
  f(x_1,\dots,x_n)=\frac{\partial^n}{\partial x_1\cdots\partial x_{n}}F(x_1,\dots,x_{n})
\end{equation*}
\subsubsection*{Función de densidad de probabilidad marginal:}
\begin{equation*}
  g_{r}(x_{r})=\int_{-\infty}^{\infty}\cdots \int_{-\infty}^{\infty}f(x_1,\dots,x_n)dx_{1}\cdots dx_{r-1}dx_{r+1}\cdots dx_{n}
\end{equation*}
\subsubsection*{Operador Esperanza:}
\begin{equation*}
  E\left[H(\vec{\mathrm{x}})\right]=\int_{-\infty}^{\infty}\cdots \int_{-\infty}^{\infty}f(x_1,\dots,x_{n})dx_{1}\cdots dx_{n}
\end{equation*}
\subsubsection*{Media de una variable en particular:}
\begin{gather*}
  E\left[\mathrm{x}_r\right]=\int_{-\infty}^{\infty}\cdots \int_{-\infty}^{\infty}x_{r}f(x_1,\dots,x_{n})dx_{1}\cdots dx_{n}\\
  E\left[\mathrm{x}_r\right]=\int_{-\infty}^{\infty}x_{r}g_{r}(x_{r})dx_{r}
\end{gather*}

\subsubsection*{Independencia de variables:}
Si las variables son independientes, tenemos
\begin{equation*}
  f(x_{1},\dots,x_{n})=g_{1}(x_{1})\cdots g_{n}(x_{n})
\end{equation*}

\subsection{Vector de media, varianza y covarianza}
Si coleccionamos todas las medias de las variables $(\mu_{\mathrm{x}_r})$ obtenemos un vector de medias dado por:
\begin{equation}
  \label{media-vec}
  \vec{\mu}_{\vec{\mathrm{x}}}=(\mu_{\mathrm{x}_1},\dots,\mu_{\mathrm{x}_n})=(E\left[\mathrm{x}_1\right]\,\cdots\,E\left[\mathrm{x}_n\right])^T
\end{equation}

\par Si aplicamos lo mismo para intentar calcular la varianza; obtenemos la \emph{matriz de varianza-covarianza}

\begin{equation*}
  E\left[(\vec{\mathrm{x}}-\vec{\mu}_{\vec{\mathrm{x}}})(\vec{\mathrm{x}}-\vec{\mu}_{\vec{\mathrm{x}}})^T\right]=\begin{bmatrix} E\left[(\mathrm{x_1}-\mu_1)^2\right] & \cdots & E\left[(\mathrm{x_1}-\mu_1)(\mathrm{x}_{n}-\mu_{n})\right]\\
    \vdots & \ddots & \vdots \\
  E\left[(\mathrm{x}_{n}-\mu_{n})(\mathrm{x_{1}}-\mu_{1})\right] & \cdots & E\left[(\mathrm{x}_{n}-\mu_{n})^2\right] \end{bmatrix}
\end{equation*}

que utilizando las definiciones de varianza y covariazna queda

\begin{equation}
  \label{eq:matriz-var-cov}
  E\left[(\vec{\mathrm{x}}-\vec{\mu}_{\vec{\mathrm{x}}})(\vec{\mathrm{x}}-\vec{\mu}_{\vec{\mathrm{x}}})^T\right]=\begin{bmatrix} \sigma^{2}_{\mathrm{x}_1} & \cdots & \mathrm{cov(x_{1},x_{n})} \\
    \vdots & \ddots & \vdots \\
    \mathrm{cov(x_{n},x_{1})} & \cdots & \sigma^{2}_{\mathrm{x}_n} \end{bmatrix}
\end{equation}

\section{Transformación de n variables}
Siendo $\vec{\mathrm{x}}=(\mathrm{x_1},\dots,\mathrm{x}_{n})$ y le aplicamos una función determinista $\vec{\mathrm{y}}$ de modo tal que,
\begin{align*}
  \mathrm{y}_1&=\mathrm{y}_1(\vec{\mathrm{x}})\\
  &\,\,\,\vdots\\ 
  \mathrm{y}_{m}&=\mathrm{y}_{m}(\vec{\mathrm{x}})
\end{align*}

\par Notar que $\vec{\mathrm{y}}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$.
\par Entonces, la función de densidad de probabilidad de la nueva variable $\vec{\mathrm{y}}$ queda
\begin{equation}
  g(\vec{y})=\left|J \left(\frac{\vec{x}}{\vec{y}}\right)\right|f(\vec{x}(\vec{y}))
  \label{eq:transfor-nvar}
\end{equation}
donde 

\begin{equation*}
  \left|J \left(\frac{\vec{x}}{\vec{y}}\right)\right|=\left|\begin{matrix} \frac{\partial x_1}{\partial y_1} & \cdots & \frac{\partial x_{n}}{\partial y_1} \\
  \vdots & \ddots & \vdots \\ 
  \frac{\partial x_{1}}{\partial y_{m}} & \cdots & \frac{\partial x_{n}}{\partial y_{m}} \end{matrix}\right|
\end{equation*}

\subsection{Transformación lineal y ortogonal}

\subsubsection{Regla de propagación de errores}
Considerando $m$ funciones lineales de $n$ variables $\vec{\mathrm{x}}: \mathbb{R}^{n\times1}$ con media y matriz de varianza-covarianza conocidas:
\begin{align*}
  \mathrm{y}_1&=a_{1}+t_{11}\mathrm{x}_{1}+\cdots+t_{1n}\mathrm{x}_{n}\\
  &\,\,\,\,\vdots\\
  \mathrm{y}_m&=a_{m}+t_{m1}\mathrm{x}_{1}+\cdots+t_{mn}\mathrm{x}_{n}\\
\end{align*}

en forma matricial o vectorial:
\begin{equation*}
  \vec{\mathrm{y}}=T\vec{\mathrm{x}}+\vec{a}
\end{equation*}

la \emph{Ley de Propagación de Errores} establece:
\begin{equation}
  \label{eq:prop-errores}
  C_{\mathrm{y}}=TC_{\mathrm{x}}T^{T}
\end{equation}

\subsubsection{Matriz T para un caso no de transformación no lineal}
En el caso de que la transformación no sea lineal, podemos \emph{linealizarla} a través de un desarrollo de Taylor a primer orden, centrado en la media de $\mathrm{x}$

\begin{equation*}
  \mathrm{y}_{i}=y_{i}(\vec{\mu})+\left(\frac{\partial y_{i}}{\partial x_{1}}\right)(\mathrm{x}_{1}-\mu_{1})+\cdots+\left(\frac{\partial y_{i}}{\partial x_{n}}\right)(\mathrm{x}_{n}-\mu_{n})+T.O.S
\end{equation*}
luego, la matriz $T$ será:

\begin{equation*}
  T=\left(\begin{matrix}\frac{\partial y_{1}}{\partial x_{1}} & \cdots & \frac{\partial y_{1}}{\partial x_{n}}\\
    \vdots & \ddots & \vdots\\
    \frac{\partial y_{m}}{\partial x_{1}} & \cdots & \frac{\partial y_{m}}{\partial x_{n}}\end{matrix}\right)
\end{equation*}

\par \textbf{Todas las derivadas son evaluadas en $\vec{\mu}$}

\chapter{Funciones de distribución especiales}
\section{Distribución Binomial}
Esta función de distribución se puede asignar a experimentos aleatorios que tengan las siguientes características:
\begin{enumerate}
  \item Sólo dos resultados posibles mutuamente excluyentes: $A$ o $\lnot A$
  \item Independencia de resultados en cada prueba.
  \item $P(A)=p=\mathrm{cte}$.
  \item Existe un número finito, $n$, de pruebas.
\end{enumerate}

La variable aleatoria binomial cuenta la cantidad de \emph{éxitos} en un total de $n$ pruebas.
\begin{equation*}
  \mathrm{x}=\sum_{i=1}^{n}\mathrm{x}_{i}
\end{equation*}
con 
\begin{equation*}
  \mathrm{x}_i = \begin{cases}
    1, & \text{si sucede}~ A\\
    0, & \text{si no sucede}~ A
  \end{cases}
\end{equation*}.

\begin{equation}
  \label{eq:prob-binomial}
  P(k)=W^{n}_{k}=\binom{n}{k}p^{k}q^{n-k}
\end{equation}

\subsection{Parámetros de la binomial}
\begin{itemize}
  \item \emph{Media:}$$E\left[\mathrm{x}_i\right]=np$$
  \item \emph{Varianza:}$$\sigma^2(\mathrm{x}_{i})=npq$$
\end{itemize}

\section{Multinomial}
Dada una serie de eventos mutuamente excluyentes que terminan definiendo todo el espacio muestral: $E=A_1+\cdots+A_{l}$; donde
\begin{equation*}
  P(A_{j})=p_{j},\quad \sum_{j=1}^{l}p_{j}=1
\end{equation*}.

La variable multinomial se expresa:
\begin{equation*}
  \mathrm{x}_{j}=\sum_{i=1}^{n}\mathrm{x}_{ij}
\end{equation*}

\begin{equation*}
  \mathrm{x}_{ij} = \begin{cases}
    1, & \text{si sucede}~ A_{j}\\
    0, & \text{en el resto.}
  \end{cases}
\end{equation*}

\par De esta manera, la probabilidad queda:
\begin{equation}
  \label{eq:prob-multinomial}
  P(\mathrm{x}_1=k_1,\dots,\mathrm{x}_l=k_{l})=\frac{n!}{\prod_{j=1}^{l}k_{j}!}\prod_{j=1}^{l}p^{k_{j}}_{j},
\end{equation}
tener en cuenta que $$\sum_{j=1}^{l}k_{j}=n$$

\subsection{Parámetros de la multinomial}
\begin{itemize}
  \item \emph{Media:}$$E\left[\vec{\mathrm{x}}\right]=n\vec{p},\quad\vec{p}=\begin{bmatrix}p_{1}\\
  \vdots\\
  p_{l}\end{bmatrix}$$
  \item \emph{Varianza:}$$\sigma^{2}(\mathrm{x}_{j})=np_{j}(1-p_{j}),\quad j=1,\dots,l$$
  \item \emph{Covarianza:}$$cov(\mathrm{x}_{i},\mathrm{x}_{j})=-np_{i}p_{j},\quad i\neq j$$
\end{itemize}

\section{Poissoniana}
Partiendo de una binomial, con las siguientes condiciones: $n\rightarrow\infty$, $p\rightarrow0$ y $np=\lambda$; tenemos

\begin{align*}
  P(k)&=\binom{n}{k}p^{k}q^ {n-k}\\
      &=\frac{n!}{k!(n-k)!}\left(\frac{\lambda}{n}\right)^ {k}\frac{\left(1-\frac{\lambda}{n}\right)^{n}}{\left(1-\frac{\lambda}{n}\right)^{k}}\\
      &=\frac{\lambda^{k}}{k!}\frac{n(n-1)(n-2)\cdots(n-k+1)}{n^{k}})\frac{\left(1-\frac{\lambda}{n}\right)^{n}}{\left(1-\frac{\lambda}{n}\right)^{k}}\\
      &=\frac{\lambda^{k}}{k!}\left(1-\frac{\lambda}{n}\right)^{n}\frac{\left(1-\frac{1}{n}\right)\cdots\left(1-\frac{k-1}{n}\right)}{\left(1-\frac{\lambda}{n}\right)^{k}},
\end{align*}
tomando $\lim_{n\rightarrow\infty}$ queda:
\begin{equation}
  \label{eq:prob-poissoniana}
  P(k)=\frac{\lambda^{k}}{k!}e^{-\lambda}
\end{equation}

\subsection{Parámetros de la poissoniana}
\begin{itemize}
  \item \emph{Media:}$$E\left[k\right]=\lambda$$,
  \item \emph{Varianza:}$$\sigma^{2}=\lambda$$.
\end{itemize}
\par \emph{Nota: Para la demostración utilizar la definición de media para variables discretas, teniendo en cuenta que la poissoniana va de 0 a $\infty$.}

\section{Gaussiana}
Es la llamada \emph{distribución normal}, su función de densidad de probabilidad está dada por:

\begin{equation*}
  f(x)=\phi(x)=\frac{1}{\sqrt{2\pi}b}\exp{\left(-\frac{(x-a)^2}{2b^2}\right)},\quad -\infty<x<\infty
\end{equation*}

\subsection{Parámetros de la gaussiana}
\begin{equation*}
  \mu=a,\qquad \sigma^2=b^2
\end{equation*}

\par \emph{Nota: para la demostración proponer cambio de variable $\mathrm{z}=(\mathrm{x}-\mu)/\sigma$} y tener en cuenta que la integral definida $\int_{-\infty}^{\infty}\exp{(-z^2/2)}dz=\sqrt{2\pi}$

\par De esta manera, la función de densidad de probabilidad queda:
\begin{equation}
  f(x)=\phi(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp{\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)},\quad -\infty<x<\infty
  \label{eq:fdp-gaussiana}
\end{equation}

\subsection{Normalización de la gaussiana}
Si utilizamos la variable normalizada $\mathrm{x}'=(\mathrm{x}-\mu)/\sigma$
\begin{equation}
  \label{eq:fdp-gaussiana-normalizada}
  \varphi(x')=\frac{1}{\sqrt{2\pi}}\exp{\left(-\frac{x'\,^2}{2}\right)}
\end{equation}

Lo importante de normalizar la gaussiana es poder encontrar valores de manera rápida, ya que se encuentran tabulados.

\subsection{Criterio de los 3 sigmas}
Utilizando al desigualdad de Chebychev \eqref{eq:Chebychev} con la gaussiana estandarizada, podemos notar lo siguiente:
\begin{gather*}
  P(|\mathrm{x}-\mu|\leq\sigma)=68.3\%,\\
  P(|\mathrm{x}-\mu|\leq2\sigma)=95.4\%,\\
  P(|\mathrm{x}-\mu|\leq3\sigma)=99.8\%.
\end{gather*}
\par De manera tal que cualquier valor aleatorio (que siga una distribución gaussiana) que se encuentre con un desvío superior a $3\sigma$ es altamente improbable. Lo cual es un buen indicador para descartar \emph{outlayers} o para encontrarlos y estudiar si son desechables o no.

\section{Método de Montecarlo}
Es un método que se basa en la generación de números \emph{pseudoaleatorios} con la intención de realizar simulaciones y realizar experimentos computacionales.

\par Para generar valores pseudoaleatorios, utilizamos una variable aleatoria $\mathrm{x}$ tal que:
\begin{equation*}
  f(x)=\begin{cases}
    1,\quad\text{Si }~0\leq x< 1;\\
    0,\quad\text{Si }~x<0\,\,\text{y}~x\geq1.
  \end{cases}
\end{equation*}
y tomando $\mathrm{y}$ como una variable aleatoria cuya función de densidad de probabilidad es $g(y)$ conocida.
\par La transformación de variables queda
\begin{equation*}
  g(y)dy=dx
\end{equation*}

\par A través de la relación de la función de distribución $G(y)$ con la función de densidad de probabilidad $g(y)$: $dG(y)/dy=g(y)$
\begin{equation*}
  dx=g(y)dy=dG(y),
\end{equation*}
lo cual, integrando queda
\begin{equation*}
  x=G(y)=\int_{-\infty}^{y}g(t)dt\,.
\end{equation*}
\par Si invertimos la variable, obtenemos:
\begin{equation}
  \label{eq:montecarlo-variable}
  \mathrm{y}=G^{-1}(\mathrm{x})
\end{equation}
lo cual implica que si se toma un número aleatorio $\mathrm{x}$ con distribución uniforme entre $0$ y $1$, obtenemos a través de $G^{-1}$ un número aleatorio $\mathrm{y}$ con función de densidad de probabilidad $g(y)$.

\subsection{Método de generación de números pseudoaleatorios}
Para generar los valores aleatorios que alimentan mi simulación de Montecarlo, necesito tener en cuenta lo siguiente:

\begin{enumerate}
  \item Distribución uniforme.
  \item Estadísticamente independientes.
  \item Su media debe ser estadísticamente igual a $0.5$.
  \item Su varianza debe ser estadísticamente igual a $0.5$.
  \item Su período\footnote{Como vamos a generarlos a través de recursiones basadas en congruencias, luego de ciertas iteraciones vuelve a dar exactamente los mismos valores} debe ser largo.
  \item Deben ser generados a través de un método computacionalmente rápido.
  \item No deben ocupar mucha memoria en la computadora.
\end{enumerate}

\par Un algoritmo muy utilizado que cumple las condiciones es el \emph{Generador Multiplicativo Congruencial Lineal} que sigue la siguiente recursión:
\begin{equation}
  \label{eq:LCG}
  x_{i}=(ax_{i-1}+c)\mod{m}\,.
\end{equation}

Como se van a utilizar números enteros de $k$ bits, se plantea:
\begin{equation}
  \label{eq:LCG-bin}
  x_{i}=(ax_{i-1}+c)\mod{2^{k}}\,.
\end{equation}

Para inicializar el generador, necesitamos determinar un valor inicial $x_0$. A partir de allí elegimos los parámetros $a$, $c$ y $k$ para producir los números que sean necesarios.

\subsection{Ejemplo}
\textrm{\large{Números aleatorios distribuidos exponencialmente.}}
\vspace{2mm}
\par Nos gustaría generar números aleatorios dados por la siguiente densidad de probabilidad
\begin{equation*}
  g(t)=\begin{cases}
    \frac{1}{\tau}e^{-t/\tau},\quad &t\geq0,\\
    0, \quad &t<0. 
  \end{cases}
\end{equation*}
\par Esta densidad de probabilidad describe el tiempo $t$ de decaimiento de un núcleo radioactivo que existe en un tiempo $t=0$ y tiene una vida media $\tau$. La función de distribución es
\begin{equation*}
  x=G(t)=\frac{1}{\tau}\int_{t'=0}^{t}g(t')dt'=1-e^{-t/\tau}\,.
\end{equation*}
\par Siguiendo las ideas del método, invertimos $G$ y obtenemos:
\begin{equation*}
  t=-\tau\ln{(1-x)},
\end{equation*}
la variable simulada $t$ por la variable $x$ generada por métodos numéricos. Por ejemplo con \refeq{eq:LCG-bin}.

\chapter{Teorema central del Límite}
\section{Función Característica}
Definimos a la función característica como
\begin{equation}
  \label{eq:func-carac-def}
  \varphi(t)=E\left[\exp(it\mathrm{x})\right]
\end{equation}

Según la naturaleza de la variable:
\begin{equation}
  \label{eq:func-carac}
  \varphi(t)=\begin{cases}
    \displaystyle\int_{-\infty}^{\infty}\exp{(itx)}f(x)dx,\quad &\mathrm{x}\,\text{continua},\\
    \\
    \displaystyle\sum_{i}\exp{(itx_{i})}P(\mathrm{x}=x_{i}),\quad &\mathrm{x}\,\text{discreta}.
  \end{cases}
\end{equation}

\par Los modos de la función característica vienen dados por:

\begin{equation}
  \label{eq:modos-func-caract}
  \varphi^{(n)}(t)=\frac{d^{n}\varphi(t)}{dt^{n}}=i^{n}\int_{-\infty}^{\infty}x^{n}\exp{(itx)}f(x)dx
\end{equation}

podemos relacionar los modos con los momentos de una variable, mediante:
\begin{equation*}
  \varphi^{(n)}(0)=i^{n}\lambda_{n}
\end{equation*}

\end{document}
